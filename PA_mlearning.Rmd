---
title: "Practical Machine Learning Prediction Assignment Writeup"
author: "Robson"
date: "18 de julho de 2016"
---

###Synopsis

In this project, we analyse data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants regarding phisical exercises with barbell lifts performed correctly and incorrectly in 5 different ways. The goal is to predict the manner in which they did the exercise. This is the "classe" variable in the training set.

###Data Processing

1. Loading the training and test data file.
```{r, eval=FALSE}
pmltraining <- read.csv("pml-training.csv")
pmltesting <- read.csv("pml-testing.csv")
```

2. Data partition between training and testing sets.
```{r, eval=FALSE}
library(caret)
inTrain <- createDataPartition(pmltraining$classe, p = 0.6)[[1]]
training <- pmltraining[ inTrain,]
testing <- pmltraining[-inTrain,]
```
60% for training and 40% for testing data sets.

3. Brief exploratory review.
```{r, eval=FALSE}
head(training)
```
As we can see, the most variables has NA's and empty values in almost all its observations. Since there is too many variables to consider in the prediction analysis, we will leave only the ones witch has values in each observation for formula composition.

4. Formula composition.
```{r, eval=FALSE}
pmlform <- as.formula(classe ~ roll_belt + pitch_belt + yaw_belt + total_accel_belt + gyros_belt_x + gyros_belt_y + gyros_belt_z + accel_belt_x + accel_belt_y + accel_belt_z + magnet_belt_x + magnet_belt_y + magnet_belt_z + roll_arm + pitch_arm + yaw_arm + total_accel_arm + gyros_arm_x + gyros_arm_y + gyros_arm_z + accel_arm_x + accel_arm_y + accel_arm_z + magnet_arm_x + magnet_arm_y + magnet_arm_z + roll_dumbbell + pitch_dumbbell + yaw_dumbbell + total_accel_dumbbell + gyros_dumbbell_x + gyros_dumbbell_y + gyros_dumbbell_z + accel_dumbbell_x + accel_dumbbell_y + accel_dumbbell_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + pitch_forearm + yaw_forearm + total_accel_forearm + gyros_forearm_x + gyros_forearm_y + gyros_forearm_z + accel_forearm_x + accel_forearm_y + accel_forearm_z + magnet_forearm_x + magnet_forearm_y + magnet_forearm_z)
```

5. Model fitting.
```{r, eval=FALSE}
library(randomForest)
set.seed(3456)
fit3 <- train(pmlform, training, method = "rf", trControl = trainControl(method = "cv"))
```
As we have 5 factor levels in our "Classe" outcome variable, we have considered, at first, using decision trees algorithm. But its accuracy was not good (about 50%). After that, we have tried the random forest algorithm as above.

###Results

1. Random forest with 10 K-Fold Cross-validation.
```{r, eval=FALSE}
fit3

Random Forest 

11776 samples
  159 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 10599, 10597, 10598, 10598, 10599, 10598, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9902345  0.9876464  0.002127344  0.002691548
  27    0.9896406  0.9868944  0.003571698  0.004519705
  52    0.9851400  0.9812009  0.004348647  0.005503144

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2.
```

2. Model testing.
```{r, eval=FALSE}
pred3 <- predict(fit3, testing)
confusionMatrix(pred3, testing$classe)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2229   12    0    0    0
         B    3 1494    3    0    0
         C    0   12 1362   26    1
         D    0    0    3 1258    2
         E    0    0    0    2 1439

Overall Statistics
                                          
               Accuracy : 0.9918          
                 95% CI : (0.9896, 0.9937)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9897          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9987   0.9842   0.9956   0.9782   0.9979
Specificity            0.9979   0.9991   0.9940   0.9992   0.9997
Pos Pred Value         0.9946   0.9960   0.9722   0.9960   0.9986
Neg Pred Value         0.9995   0.9962   0.9991   0.9957   0.9995
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2841   0.1904   0.1736   0.1603   0.1834
Detection Prevalence   0.2856   0.1912   0.1786   0.1610   0.1837
Balanced Accuracy      0.9983   0.9916   0.9948   0.9887   0.9988
```
**As we can see, the model passed in the test results since the accuracy is approximately the same. Therefore, we are ready to predict the 20 pmltesting test cases with expected model accuracy of 99%**